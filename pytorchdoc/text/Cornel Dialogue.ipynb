{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cornel Chatbot dialogue\n",
    "The code below insipired by \n",
    "1. https://pytorch.org/tutorials/beginner/chatbot_tutorial.html\n",
    "2. https://github.com/Currie32/Chatbot-from-Movie-Dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys module\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import unicodedata\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "# third parties module\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "path_this = os.path.abspath (os.path.dirname ('.'))\n",
    "path_lines = os.path.join (path_this, '..', 'data', 'cornell movie-dialogs corpus', 'movie_lines.txt')\n",
    "path_conversation = os.path.join (path_this, '..', 'data', 'cornell movie-dialogs corpus', 'movie_conversations.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 304713 keys\n"
     ]
    }
   ],
   "source": [
    "# load lines\n",
    "id2lines = {}\n",
    "with open (path_lines, 'rb') as f_:\n",
    "    lines = [l.decode ('utf8', 'ignore') for l in f_.readlines()]\n",
    "    for l in lines:\n",
    "        entry = [_.strip () for _ in l.split ('+++$+++')]\n",
    "        id2lines[entry[0]] = entry[-1]\n",
    "print (\"Loaded {} keys\".format (len (id2lines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 221616 conversations\n",
      "Sample conversation: \n",
      "id : 193392\n",
      "Q: I <u>can</u>?\n",
      "A: You bet your life.  \"The mill wheel goes around...some times it's even under water -- then it rises up, as high as it can go...\"\n"
     ]
    }
   ],
   "source": [
    "# load conversation\n",
    "conversation = []\n",
    "with open (path_conversation, 'rb') as f_:\n",
    "    lines = [l.decode ('utf8', 'ignore') for l in f_.readlines ()]\n",
    "    for l in lines:\n",
    "        entry = [_.strip () for _ in l.split (\"+++$+++\")]\n",
    "        conv = entry[-1][1:-1].replace (' ', '').replace (\"'\", \"\").split (',')\n",
    "        for i in range (len (conv) - 1):\n",
    "            conversation.append ((conv[i], conv[i+1]))\n",
    "print (\"We have {} conversations\".format (len (conversation)))\n",
    "print (\"Sample conversation: \")\n",
    "\n",
    "def sample_conversation (id=None):\n",
    "    if id is None: id = random.randrange (len (conversation))\n",
    "    print (\"id : {}\".format (id))\n",
    "    sample = conversation[id]\n",
    "    print (\"Q: {}\".format (id2lines[sample[0]]))\n",
    "    print (\"A: {}\".format (id2lines[sample[1]]))\n",
    "sample_conversation ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary class\n",
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "class Vocabulary:\n",
    "    \n",
    "    def __init__ (self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {\"PAD\" : 0, \"SOS\" : 1, \"EOS\" : 2}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0:\"PAD\", 1:\"SOS\", 2:\"EOS\"}\n",
    "        self.num_words = 3\n",
    "            \n",
    "    def add_word (self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    \n",
    "    def add_sentence (self, sentence):\n",
    "        for word in sentence.split (\" \"):\n",
    "            self.add_word (word)\n",
    "    \n",
    "    # remove word below certain frequencies\n",
    "    def trim (self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "                \n",
    "        keep_words = []\n",
    "        \n",
    "        for k,v in self.word2count.items ():\n",
    "            if v >= min_count:\n",
    "                keep_words.append (k)\n",
    "        print (\"From {}, {} are kept\".format (self.num_words, len (keep_words)))\n",
    "        \n",
    "        # redefine attribute\n",
    "        self.__init__ (self.name)\n",
    "        for w in keep_words:\n",
    "            self.add_word (w)\n",
    "        self.trimmed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert line in id2lines into ASCII and then remove any line and any conversation\n",
    "# that has total word greater than thres\n",
    "def unicode2ascii (text):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(text):\n",
    "    s = unicode2ascii(text.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def prepare_data (id2lines, conversation, max_length=10):\n",
    "    for k,v in id2lines.items ():\n",
    "        v = normalize_string (v)\n",
    "        if len (v.split (\" \")) > max_length:\n",
    "            id2lines[k] = \"\"\n",
    "        else:\n",
    "            id2lines[k] = v\n",
    "    \n",
    "    clean_conversation = []\n",
    "    for conv_idx, conv in enumerate (conversation):\n",
    "        if not any ([id2lines[i] == \"\" for i in conv]):\n",
    "            clean_conversation.append(conv)\n",
    "    print (\"From {}, trimmed to {}\".format (len (conversation), len (clean_conversation)))\n",
    "    conversation = clean_conversation\n",
    "    clean_conversation = None # free the memory\n",
    "    return id2lines, conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 221616, trimmed to 75000\n"
     ]
    }
   ],
   "source": [
    "id2lines, conversation = prepare_data (id2lines, conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id : 48306\n",
      "Q: hold on a second !\n",
      "A: look at this it s so lean and clean .\n"
     ]
    }
   ],
   "source": [
    "sample_conversation ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words 20164\n",
      "From 20164, 9041 are kept\n"
     ]
    }
   ],
   "source": [
    "# create vocabulary\n",
    "vocab = Vocabulary (\"cornell\")\n",
    "for c in conversation:\n",
    "    vocab.add_sentence (id2lines[c[0]])\n",
    "    vocab.add_sentence (id2lines[c[1]])\n",
    "print (\"Total words {}\".format (vocab.num_words))\n",
    "vocab.trim (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we remove some vocabulary, we also need to check the conversation, \n",
    "# remove any conversation that contain removed word\n",
    "def trim_conversation (id2lines, conversation, vocab):\n",
    "    for _id,line in id2lines.items ():\n",
    "        word = line.split (\" \")\n",
    "        is_removed = any ([w not in vocab.word2index for w in word])\n",
    "        if is_removed:\n",
    "            id2lines[_id] = None\n",
    "\n",
    "    # actually removing the conversation\n",
    "    clean_conversation = []\n",
    "    for c in conversation:\n",
    "        if all ([id2lines[_] is not None for _ in c]):\n",
    "            clean_conversation.append (c)\n",
    "    conversation = clean_conversation\n",
    "    clean_conversation = None\n",
    "\n",
    "    return id2lines, conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before trimed : 75000 conversations\n",
      "After trimed : 62721 conversations\n"
     ]
    }
   ],
   "source": [
    "print (\"Before trimed : {} conversations\".format (len (conversation)))\n",
    "id2lines, conversation = trim_conversation (id2lines, conversation, vocab)\n",
    "print (\"After trimed : {} conversations\".format (len (conversation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id : 18425\n",
      "Q: how d you know i d do it .\n",
      "A: do what ?\n"
     ]
    }
   ],
   "source": [
    "sample_conversation ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To be able to do minibatches of seq2seq, we need to convert a bunch\n",
    "of dataset into (max_length, batch) matrix\n",
    "\n",
    "Notes on why we convert it into (max_length, batch), not (batch, max_length)\n",
    "as all we know it. The reason for this is the inner working of RNN\n",
    "is the same as sequence model, it iteratively pick the first item, run it\n",
    "and then continue to the next one. If we are using (batch, max_length), then\n",
    "our matrix will be \n",
    "    [\n",
    "        <tensor doc 1>\n",
    "        <tensor doc 2>\n",
    "        ...\n",
    "        <tensor doc n>\n",
    "    ]\n",
    "if we index it by it's first dimension, (i.e matrix[0]), then we will get the whole\n",
    "tensor document. And we don't want it (because we want to step by step).\n",
    "So our matrix should be\n",
    "    [\n",
    "        <all index 0 of doc>\n",
    "        <all index 1 of doc>\n",
    "        ...\n",
    "    ]\n",
    "so, in the first iteration, matrix[0] will return all the first word of all documents,\n",
    "and then it can continue to the second word, third word, etc.\n",
    "\n",
    "The conclusion in here is: inner pytorch iteratively access fist dimension, \n",
    "we don't want operation per doc, but per time stamp.\n",
    "\"\"\"\n",
    "\n",
    "def sentence2index (sentence, vocab):\n",
    "    return [vocab.word2index[w] for w in sentence.split (\" \")] + [vocab.word2index[\"EOS\"]]\n",
    "\n",
    "def zero_padding (m, vocab):\n",
    "    # in here we transpose from (batch_size, max_length) into (max_length, batch_size)\n",
    "    return list (itertools.zip_longest (*m, fillvalue=vocab.word2index['PAD']))\n",
    "\n",
    "def binary_mask (m, vocab):\n",
    "    mask = []\n",
    "    for idx, l in enumerate (m):\n",
    "        mask.append ([])\n",
    "        for w in l:\n",
    "            if w == vocab.word2index['PAD']:\n",
    "                mask[-1].append (0)\n",
    "            else:\n",
    "                mask[-1].append (1)\n",
    "    return mask\n",
    "\n",
    "def transform_input (sentences, vocab):\n",
    "    # input is a string word\n",
    "    s_index= [sentence2index (s, vocab) for s in sentences] # convert to index\n",
    "    lengths = torch.tensor ([len (s) for s in s_index]) # get length\n",
    "    \n",
    "    s_index_padded = torch.LongTensor (zero_padding (s_index, vocab)) # padding, convert to tensor\n",
    "    return s_index_padded, lengths\n",
    "\n",
    "def transform_output (sentences, vocab):\n",
    "    # input is a string word\n",
    "    s_index = [sentence2index (s, vocab) for s in sentences] # convert to index\n",
    "    max_length = max ([len (s) for s in s_index]) # get maximum length of this batches\n",
    "    s_index_padded = zero_padding (s_index, vocab)\n",
    "    \n",
    "    mask = torch.ByteTensor (binary_mask (s_index_padded, vocab)) # mask, convert to tensor\n",
    "    s_index_padded = torch.LongTensor (s_index_padded) # convert to tensor\n",
    "    \n",
    "    return s_index_padded, max_length, mask    \n",
    "\n",
    "def batch2tensor (pair_batch, vocab):\n",
    "    pair_batch.sort (key=lambda x : len (x[0].split (\" \")), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append (pair[0]) # question\n",
    "        output_batch.append (pair[1]) # its answer\n",
    "    \n",
    "    input_tensor, lengths = transform_input (input_batch, vocab)\n",
    "    output_tensor, max_length, mask = transform_output (output_batch, vocab)\n",
    "    \n",
    "    return input_tensor, output_tensor, lengths, max_length, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example validation\n",
    "batch_size = 5\n",
    "to_line = lambda conv: [id2lines[conv[0]], id2lines[conv[1]]]\n",
    "batches_pair = [to_line (conversation[random.randrange (len (conversation))]) for i in range (batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['huh ?', 'got another song for us ?'],\n",
       " ['a dead body ?', 'it s amy kramer .'],\n",
       " ['access granted . male or female ?', 'male .'],\n",
       " ['bud . . .', 'yeah . . .'],\n",
       " ['do you have children ?', 'no .']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, output_tensor, lengths, max_length, mask = batch2tensor (batches_pair, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[5803,   54,    8,  954,   32],\n",
       "        [3129,   17,  215,   11,   16],\n",
       "        [  11,   18, 1696,   11,    2],\n",
       "        [2971, 3278,   16,   11,    0],\n",
       "        [ 354,   16,    2,    2,    0],\n",
       "        [2192,    2,    0,    0,    0],\n",
       "        [  16,    0,    0,    0,    0],\n",
       "        [   2,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (input_tensor.shape) # (max_length, batch_size)\n",
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2971,   43,    6,  196,  594],\n",
       "        [  11,   11,    4,   11, 1032],\n",
       "        [   2,    2, 1035,   11,  991],\n",
       "        [   0,    0, 4426,   11,  119],\n",
       "        [   0,    0,   11,    2,  424],\n",
       "        [   0,    0,    2,    0,   16],\n",
       "        [   0,    0,    0,    0,    2]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1],\n",
       "         [0, 0, 1, 1, 1],\n",
       "         [0, 0, 1, 1, 1],\n",
       "         [0, 0, 1, 0, 1],\n",
       "         [0, 0, 0, 0, 1]], dtype=torch.uint8), 7)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask, max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Encoder Bidirectional RNN\n",
    "Is a 2 normal RNN but the other RNN is operated on reversed. Each input goes to the network independently, then the output of each RNN is summed.\n",
    "\n",
    "Computational graph:\n",
    "1. Convert input into word embedding\n",
    "2. Pack the sequence\n",
    "3. Pass to RNN\n",
    "4. Unpack the result\n",
    "5. Sum the result of the 2 RNN\n",
    "6. Return output and the final hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN (nn.Module):\n",
    "    \n",
    "    def __init__ (self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super (EncoderRNN, self).__init__ ()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # the embedding output must be hidden_size too\n",
    "        self.gru = nn.GRU (hidden_size, hidden_size, n_layers, \n",
    "                           dropout= (0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "    \n",
    "    def forward (self, input_seq, input_lengths, hidden=None):\n",
    "        # what is the input sequence length? is it padded already?\n",
    "        embed = self.embedding (input_seq) \n",
    "        # padding and packing\n",
    "        pack = nn.utils.rnn.pack_padded_sequence (embed, input_lengths) \n",
    "        outputs, hidden = self.gru (pack, hidden)\n",
    "        # reverse the pack_padded\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence (outputs)\n",
    "        # since bidirectional is true, we sum it\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luong attention layer\n",
    "class Attn (nn.Module):\n",
    "    \n",
    "    def __init__ (self, method, hidden_size):\n",
    "        super (Attn, self).__init__ ()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError (\"{} is not accepted method\".format (self.method))\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear (self.hidden_size, self.hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear (2*self.hidden_size, self.hidden_size)\n",
    "            self.v = nn.Parameter (torch.FloatTensor (self.hidden_size))\n",
    "    \n",
    "    def dot_score (self, hidden, encoder_output):\n",
    "        # become max_seq, batch_size\n",
    "        return torch.sum (hidden * encoder_output, dim=2)\n",
    "    \n",
    "    def general_score (self, hidden, encoder_output):\n",
    "        energy = self.attn (encoder_output)\n",
    "        return torch.sum (hidden * energy, dim=2)\n",
    "    \n",
    "    def concat_score (self, hidden, encoder_output):\n",
    "        energy = self.attn (\n",
    "            torch.cat (\n",
    "                (hidden.expand (encoder_output.size (0), -1, -1), encoder_output),\n",
    "                2\n",
    "            )).tanh ()\n",
    "        return torch.sum (self.v * energy, dim=2)\n",
    "    \n",
    "    def forward (self, hidden, encoder_outputs):\n",
    "        # hidden -> (1, batch, hidden)\n",
    "        # encoder_output -> (seq_length, batch, hidden)\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score (hidden, encoder_output)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score (hidden, encoder_output)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score (hidden, encoder_outputs)\n",
    "        \n",
    "        # attn_energies -> (batch_size, max_seq)\n",
    "        attn_energies = attn_energies.t () # transpose\n",
    "        \n",
    "        # return (batch_size, 1, max_seq)?\n",
    "        return F.softmax (attn_energies, dim=1).unsqueeze (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Decoder With Undirectional GRU with Attention\n",
    "\n",
    "Computation Graph:\n",
    "1. Get embedding of current input word\n",
    "2. Forward through undirectional GRU\n",
    "3. Calculate attention weights based on current GRU output (2) and encoder output\n",
    "4. Multiply attention weights to encoder output to get new \"weighted sum\" context vector.\n",
    "5. Concatenate weighted context and GRU output\n",
    "6. Predict next word\n",
    "7. Return output and final hidden state\n",
    "\n",
    "based on that we have\n",
    "\n",
    "**inputs** :\n",
    "- `input_step`, one time step (one word) of input sequence batch, *shape=(1, batch_size)*\n",
    "- `last_hidden`, the last hidden of GRU, *shape=(n_layers x num_direction, batch_size, hidden_size)*\n",
    "- `encoder_outputs`, the output of encoding process, *shape=(max_length, batch_size, hidden_size)*\n",
    "\n",
    "**outputs**:\n",
    "- `output`, softmax normalized tensor given the probability of each word output, *shape=(batch_size, voc.num_words)*\n",
    "- `hidden`: hidden state of GRU, *shape=(n_layers x directions, batch_size, hidden_size)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN (nn.Module):\n",
    "    \n",
    "    def __init__ (self, attn_model, embedding,\n",
    "                 hidden_size, output_size, \n",
    "                  n_layers=1, dropout=0.1):\n",
    "        \n",
    "        super (AttnDecoderRNN, self).__init__ ()\n",
    "        \n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # define layers needed\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout (self.dropout)\n",
    "        self.gru = nn.GRU (hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers==1 else dropout))\n",
    "        \n",
    "        # for joining context vector and decoder hidden state\n",
    "        self.concat = nn.Linear (hidden_size*2, hidden_size)\n",
    "        # from the concat into the word prob\n",
    "        self.out = nn.Linear (hidden_size, output_size)\n",
    "        \n",
    "        self.attn = Attn (attn_model, hidden_size)\n",
    "        \n",
    "    def forward (self, input_step, last_hidden, encoder_outputs):\n",
    "        # note: input_step is the index of word with just one word\n",
    "        # so the shape is (1, batch_size).\n",
    "        embed = self.embedding (input_step)\n",
    "        embed = self.embedding_dropout (embed)\n",
    "        \n",
    "        # forward through the RNN\n",
    "        # RNN_output --> (seq_length, batch, num_direction*hidden_size) = (1, batch, hidden_size)\n",
    "        rnn_output, hidden = self.gru (embed, last_hidden)\n",
    "        \n",
    "        # calculate attention\n",
    "        # encoder_output --> (seq_length, batch, num_direction*hidden_size) = (seq_length, batch, hidden_size)\n",
    "        # we summed the bidirectional encoder output right?\n",
    "        attn_weights = self.attn (rnn_output, encoder_outputs)\n",
    "        # attn_weights -> (batch, 1, seq_length)\n",
    "        \n",
    "        # multiply the weight with encoder output to get the context\n",
    "        # encoder_outputs must be (batch, seq_length, hidden), so we transpose it from (seq_length, batch, hidden)\n",
    "        # in other words, from index 0 to index 1\n",
    "        context = attn_weights.bmm (encoder_outputs.transpose (0,1))\n",
    "        # context will be (batch, 1, hidden)\n",
    "        \n",
    "        # concat the context vector with rnn output\n",
    "        rnn_output = rnn_output.squeeze (0)\n",
    "        context = context.squeeze (1)\n",
    "        concat_input = torch.cat ((rnn_output, context), 1)\n",
    "        \n",
    "        # put into tanh of concat layer network\n",
    "        concat_output = torch.tanh (self.concat (concat_input))\n",
    "        \n",
    "        # predict the output\n",
    "        output = self.out (concat_output)\n",
    "        output = F.softmax (output, dim=1)\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So based on my understanding, the attention eventually will produce tensor with shape:\n",
    "\n",
    "**(batch_size, seq_length)**\n",
    "\n",
    "And then, this attention will be multiplied with `encoder_output` to produce context with size **(batch_size, hiden)**\n",
    "\n",
    "After that, this context vector will be joined with RNN output, and then feed into network with layer (2 x hidden_size, hidden_size)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Procedure\n",
    "\n",
    "### Masked Loss\n",
    "Since we are dealing with padded sequence, we can't simply consider all elements while calculating loss. Only element that has a word in it we should consider. Therefore we need to create a mask loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Training Iteration\n",
    "\n",
    "For a single batch of input, we define a `train` function that will take the input, process it and count it's gradient.\n",
    "In here we will use a couple techniques, such as **teacher forcing** and **gradient clipping**\n",
    "\n",
    "The algorith of this function is:\n",
    "1. **Forward** `input` batch through encoder.\n",
    "2. **Initialize** decoder input as SOS_token and hidden_state as the encoder's final hidden state.\n",
    "3. Forward input batch sequence **through decoder** one time step at a time\n",
    "4. If **teacher forcing**: set next decoder input as the current target, else set as current decoder output\n",
    "5. Calculate and accumulate **loss**\n",
    "6. Perform **backpropagation**\n",
    "7. **Clip** the gradients\n",
    "8. **Update** encoder and decoder parameter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
